# íŒŒì¼ ì´ë¦„: final-analysis.py (ëª¨ë“  ë‹¨ê³„ í†µí•© - ìµœì¢… ë²„ì „)

# 1. í™˜ê²½ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ
from dotenv import load_dotenv
import os
import json
import time
from typing import List
from pydantic import BaseModel, Field

# LangChain ë° Google GenAI ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser # ìˆ˜ì •: ìµœì‹  ëª¨ë“ˆ ê²½ë¡œ ì‚¬ìš©
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough

# --- Pydantic ìŠ¤í‚¤ë§ˆ ì •ì˜ ---
class EmotionTag(BaseModel):
    """ì¼ê¸° í•œ ë¬¸ë‹¨ ë˜ëŠ” ì²­í¬ì—ì„œ ì¶”ì¶œëœ í•˜ë‚˜ì˜ ê°ì • íƒœê·¸."""
    emotion: str = Field(description="êµ¬ì²´ì ì¸ ê°ì • ì´ë¦„ (ê¸ì •, ë¶ˆì•ˆ, ë¶„ë…¸, ìŠ¬í”” ë“±).")
    intensity: float = Field(description="ê°ì •ì˜ ê°•ë„ (0.0ì—ì„œ 1.0 ì‚¬ì´).")
    reason: str = Field(description="ì´ ê°ì •ì„ ëŠë¼ê²Œ ëœ êµ¬ì²´ì ì¸ ì‚¬ê±´ì´ë‚˜ ë¬¸êµ¬.")

class EmotionAnalysisReport(BaseModel):
    """í•˜ë‚˜ì˜ ì¼ê¸° ì²­í¬ì— ëŒ€í•œ ê°ì • ë¶„ì„ ì‹¬í™” ë³´ê³ ì„œ."""
    summary: str = Field(description="í•´ë‹¹ ì¼ê¸° ì²­í¬ì˜ í•µì‹¬ ë‚´ìš© ìš”ì•½ (30ì ì´ë‚´).")
    emotion_tags: List[EmotionTag] = Field(description="ì¼ê¸° ì²­í¬ì—ì„œ ë°œê²¬ëœ ëª¨ë“  ê°ì • íƒœê·¸ ëª©ë¡.")
# -----------------------------

# 2. ë°ì´í„° ë¡œë”© ë° ë¶„í• 
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEYë¥¼ .env íŒŒì¼ì— ì •í™•íˆ ì…ë ¥í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

file_path = "./data_raw/my-diaries-7days.txt"

# íŒŒì¼ ì¡´ì¬ í™•ì¸ (ì´ ì½”ë“œëŠ” ì£¼ì„ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤.)
if not os.path.exists(file_path):
    print(f"âŒ ì˜¤ë¥˜: ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {file_path}")
    exit()

loader = TextLoader(file_path, encoding='utf-8')
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(
    separators=["---", "\n\n", "\n", " "],
    chunk_size=1000,
    chunk_overlap=0,
)
chunks = text_splitter.split_documents(documents)

processed_documents = []
for i, chunk in enumerate(chunks):
    chunk.metadata['doc_type'] = 'diary_entry'
    chunk.metadata['entry_id'] = i + 1
    processed_documents.append(chunk)

print(f"ğŸ‰ ë°ì´í„° ë¡œë”© ë° ë¶„í•  ì™„ë£Œ! ì´ {len(processed_documents)}ê°œì˜ Document ê°ì²´ ìƒì„±.")


# 3. LLM ë¶„ì„ ì²´ì¸ ì„¤ì •
parser = PydanticOutputParser(pydantic_object=EmotionAnalysisReport)

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=GEMINI_API_KEY,
    temperature=0.1,
)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            (
                "ë‹¹ì‹ ì€ ì‹¬ë¦¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¼ê¸°ë¥¼ ë¶„ì„í•˜ì—¬ ê°ì • ìœ í˜•, ê°•ë„(0.0~1.0), ê·¸ë¦¬ê³  ì›ì¸ ì‚¬ê±´ì„ ì¶”ì¶œí•˜ì„¸ìš”. "
                "ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì— ë§ì¶°ì„œ JSONìœ¼ë¡œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.\n"
                "{format_instructions}"
            ),
        ),
        ("human", "ë‹¤ìŒ ì¼ê¸°ë¥¼ ë¶„ì„í•˜ì—¬ ìƒì„¸ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”:\n\n{diary_chunk}"),
    ]
)
emotion_chain = prompt | llm | parser


# 4. ì¼ê´„ ë¶„ì„ ë° ì €ì¥
print("\n==============================================")
print("ğŸ§  ì¼ê´„ ê°ì • ë¶„ì„ ì‹œì‘: ëª¨ë“  ì¼ê¸° ì²­í¬ ë¶„ì„ ì¤‘...")
print("==============================================")

all_analysis_reports = []

for i, chunk in enumerate(processed_documents):
    entry_id = chunk.metadata.get('entry_id', 'Unknown')
    print(f"--- [ë¶„ì„ ì¤‘] ì²­í¬ ë²ˆí˜¸: {i+1}/{len(processed_documents)} ---")
    
    try:
        analysis_result = emotion_chain.invoke(
            {
                "diary_chunk": chunk.page_content,
                "format_instructions": parser.get_format_instructions(),
            }
        )
        report_data = analysis_result.model_dump()
        report_data['metadata'] = chunk.metadata
        all_analysis_reports.append(report_data)
        print(f"âœ… ë¶„ì„ ì™„ë£Œ! (ID: {entry_id})")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì˜¤ë¥˜ ë°œìƒ (ID: {entry_id}): {e}")
    
    time.sleep(1) 

print("\nğŸ‰ ëª¨ë“  ì¼ê¸° ë¶„ì„ ì™„ë£Œ!")
output_file_path = "./emotion-reports.json" # íŒŒì¼ëª… í•˜ì´í”ˆ ì ìš©
try:
    with open(output_file_path, 'w', encoding='utf-8') as f:
        json.dump(all_analysis_reports, f, ensure_ascii=False, indent=4)
    print(f"âœ… ìµœì¢… ë³´ê³ ì„œ ì €ì¥ ì„±ê³µ: {output_file_path}")
except Exception as e:
    print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜ ë°œìƒ: {e}")


# 5. ì¢…í•© ì‹¬ë¦¬ ë³´ê³ ì„œ ìƒì„± ë° ì €ì¥
reports_string = json.dumps(all_analysis_reports, ensure_ascii=False, indent=2)

report_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            (
                "ë‹¹ì‹ ì€ ì‹¬ë¦¬ ë¶„ì„ê°€ì´ë©°, ì œê³µëœ ì¼ê¸° ë¶„ì„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¢…í•©ì ì¸ ì‹¬ë¦¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. "
                "ë³´ê³ ì„œëŠ” í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë©°, ì£¼ìš” ê°ì • íŒ¨í„´ê³¼ ê·¸ ìš”ì¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  ê³µê°ì ì¸ ì–´ì¡°ë¡œ ì„œìˆ í•˜ì„¸ìš”. "
                "ëª©ì°¨ì™€ ì†Œì œëª©(##)ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°ë¥¼ ëª…í™•í•˜ê²Œ ë§Œë“œì„¸ìš”."
            ),
        ),
        ("human", "ë‹¤ìŒì€ ì œ ì¼ê¸° ë¶„ì„ ê²°ê³¼(JSON)ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•©í•˜ì—¬ ì¢…í•© ì‹¬ë¦¬ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”:\n\n{analysis_data}"),
    ]
)
report_chain = report_prompt | llm

print("\n==============================================")
print("ğŸ“ ì¢…í•© ì‹¬ë¦¬ ë³´ê³ ì„œ ìƒì„± ì‹œì‘...")
print("==============================================")

try:
    final_report = report_chain.invoke(
        {
            "analysis_data": reports_string,
        }
    )

    print("âœ… ì¢…í•© ë³´ê³ ì„œ ìƒì„± ì„±ê³µ!")
    print("\n--- ìµœì¢… ì‹¬ë¦¬ ë³´ê³ ì„œ ---")
    report_content = final_report.content
    print(report_content)
    print("-------------------------\n")
    
    report_output_file = "final-psychological-report.md" # íŒŒì¼ëª… í•˜ì´í”ˆ ì ìš©
    with open(report_output_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"âœ… ë³´ê³ ì„œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {report_output_file}")
    
except Exception as e:
    print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# 6. RAG ì‹œìŠ¤í…œ êµ¬ì¶• ë° í…ŒìŠ¤íŠ¸
print("\n==============================================")
print("ğŸ” RAG ì‹œìŠ¤í…œ êµ¬ì¶• ë° í…ŒìŠ¤íŠ¸ ì‹œì‘...")
print("==============================================")

try:
    embeddings = GoogleGenerativeAIEmbeddings(
        model="models/embedding-001",
        api_key=GEMINI_API_KEY
    )

    vectorstore = Chroma.from_documents(
        documents=processed_documents, 
        embedding=embeddings
    )

    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", ("ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì¼ê¸° ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì „ë¬¸ ê²€ìƒ‰ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. "
                        "ì£¼ì–´ì§„ 'ë§¥ë½ ì •ë³´'ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ë‹µì´ ì—†ë‹¤ë©´, 'ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
                        "\n\n--- ë§¥ë½ ì •ë³´ ---\n{context}")),
            ("human", "ì§ˆë¬¸: {question}"),
        ]
    )

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | rag_prompt
        | llm
    )

    test_question = "ë‚´ê°€ ì¼ì£¼ì¼ ë™ì•ˆ ê°€ì¥ ê¸°ë»¤ë˜ ì‚¬ê±´ì€ ë¬´ì—‡ì´ë©°, ê·¸ ë‚ ì§œëŠ” ì–¸ì œì•¼?"

    print(f"**í…ŒìŠ¤íŠ¸ ì§ˆë¬¸:** {test_question}")

    rag_response = rag_chain.invoke({"question": test_question})
        
    print("\n--- RAG ë‹µë³€ ---")
    print(rag_response.content)
    print("------------------\n")
        
    vectorstore.delete_collection()
    print("âœ… RAG ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ. ë©”ëª¨ë¦¬ ì •ë¦¬.")

except Exception as e:
    print(f"âŒ RAG ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("Embedding ëª¨ë¸(embedding-001) ê¶Œí•œ ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
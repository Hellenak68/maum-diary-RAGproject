# íŒŒì¼ ì´ë¦„: data_crawler.py (ê°œì„ ëœ ë²„ì „)

import requests
from bs4 import BeautifulSoup
import time
import os # í´ë” ê´€ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€

# ==========================================================
# ğŸš¨ğŸš¨ ì—¬ê¸°ë¥¼ ë„¤ ì •ë³´ë¡œ ë‹¤ì‹œ ì •í™•íˆ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤! ğŸš¨ğŸš¨
# ==========================================================
BLOG_ID = "kobau68"
START_POST_NUM = 224087453042 # 6ê°œì›” ì „ í¬ìŠ¤íŠ¸ ë²ˆí˜¸
END_POST_NUM = 2224095240255   # ê°€ì¥ ìµœê·¼ ê¸€ ë²ˆí˜¸
# ==========================================================


def extract_post_data(post_num):
    """ë‹¨ì¼ í¬ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ, ì œëª©, ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    url = f"https://blog.naver.com/PostView.naver?blogId={BLOG_ID}&logNo={post_num}"
    
    # User-Agent ì¶”ê°€: 'ë‚˜ëŠ” ì›¹ ë¸Œë¼ìš°ì €ë‹¤'ë¼ê³  ë„¤ì´ë²„ì— ì•Œë ¤ì¤ë‹ˆë‹¤.
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        # í—¤ë”ë¥¼ í¬í•¨í•˜ì—¬ ìš”ì²­í•©ë‹ˆë‹¤.
        response = requests.get(url, headers=headers) 
        response.raise_for_status() 
        
        soup = BeautifulSoup(response.text, 'html.parser')

        # [í•„ìˆ˜ ê²€í† ] ë§Œì•½ ì—¬ê¸°ì„œ ì˜¤ë¥˜ê°€ ë‚˜ë©´ ë„¤ì´ë²„ ë¸”ë¡œê·¸ ë””ìì¸ì´ ë°”ë€Œì—ˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.
        title_element = soup.select_one('.se-viewer .se-title-text')
        date_element = soup.select_one('.se-viewer .date-info')
        
        # ìš”ì†Œê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
        if not title_element or not date_element:
            print(f"âŒ ì‹¤íŒ¨: {post_num}ë²ˆ ê¸€ì€ í˜•ì‹ì´ ë§ì§€ ì•Šê±°ë‚˜ ë¹„ê³µê°œì…ë‹ˆë‹¤.")
            return None

        title = title_element.text.strip()
        date = date_element.text.strip()
        
        content_paragraphs = [p.text for p in soup.select('.se-main-container p')]
        content = "\n".join(content_paragraphs)
        
        print(f"âœ… ì„±ê³µ: {post_num}ë²ˆ ê¸€ ({title})")
        return f"ë‚ ì§œ: {date}\nì œëª©: {title}\në³¸ë¬¸:\n{content}\n\n---\n\n"
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {post_num}ë²ˆ ê¸€ - {e}")
        return None

# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---

# í´ë”ê°€ ì—†ìœ¼ë©´ ë§Œë“­ë‹ˆë‹¤. (ì›ì¸ 1 í•´ê²°)
output_dir = "./data_raw"
os.makedirs(output_dir, exist_ok=True) 

all_diaries = ""
# ì‹œì‘ ë²ˆí˜¸ë¶€í„° ë ë²ˆí˜¸ê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤. (ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•´ 2ì´ˆì”© ì‰¬ë„ë¡ ìˆ˜ì •í•©ë‹ˆë‹¤.)
for num in range(START_POST_NUM, END_POST_NUM + 1):
    diary_text = extract_post_data(num)
    if diary_text:
        all_diaries += diary_text
    
    time.sleep(2) # ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•´ 2ì´ˆì”© ì‰½ë‹ˆë‹¤.

# ìµœì¢…ì ìœ¼ë¡œ ì¶”ì¶œëœ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
output_file = os.path.join(output_dir, "my_diaries_6months.txt")
with open(output_file, "w", encoding="utf-8") as f:
    f.write(all_diaries)

print("\n==============================================")
print(f"ğŸ‰ ì¶”ì¶œ ì™„ë£Œ! {output_file} íŒŒì¼ í™•ì¸.")
print("==============================================")